name: Compare with pytest

on:
  push:
    branches: [ main, develop, fix/* ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC to track performance trends
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_size:
        description: 'Test suite size for comparison'
        required: false
        default: '100'
        type: choice
        options:
        - '10'
        - '50'
        - '100'
        - '500'
        - '1000'

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always

jobs:
  compare-performance:
    name: Performance Comparison
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.8', '3.11', '3.12']
        test-size: [10, 50, 100]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache Rust dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark matplotlib pandas
        
    - name: Build fastest
      run: |
        cargo build --release
        chmod +x target/release/fastest-cli
        
    - name: Create test directories
      run: |
        mkdir -p comparison_results performance_data tests/compatibility
        
    - name: Run performance comparison
      run: |
        # Make scripts executable
        chmod +x scripts/compare_with_pytest.py
        chmod +x scripts/track_performance_regression.py
        
        # Create sample tests of specified size
        python scripts/compare_with_pytest.py --create-sample ${{ matrix.test-size }} --save-results
        
        # Run comparison with the created tests
        TEMP_DIR=$(python scripts/compare_with_pytest.py --create-sample ${{ matrix.test-size }} | grep "Created:" | awk '{print $2}')
        python scripts/compare_with_pytest.py "$TEMP_DIR" --fastest-binary ./target/release/fastest-cli --save-results
        
        # Track performance metrics
        python scripts/track_performance_regression.py --binary ./target/release/fastest-cli --test-dir "$TEMP_DIR" --runs 3
        
    - name: Generate comparison report
      run: |
        # Generate performance report
        python scripts/track_performance_regression.py --report > performance_report.md
        
        # Add results to summary
        echo "## Performance Comparison Results" >> $GITHUB_STEP_SUMMARY
        echo "Python: ${{ matrix.python-version }}, Test Size: ${{ matrix.test-size }}" >> $GITHUB_STEP_SUMMARY
        
        # Extract key metrics from latest comparison
        if [ -f "comparison_results/latest.json" ]; then
          echo "### Quick Stats" >> $GITHUB_STEP_SUMMARY
          python -c "
import json
with open('comparison_results/latest.json', 'r') as f:
    data = json.load(f)
    
perf = data.get('performance_ratio', {})
compat = data.get('compatibility_analysis', {})

print(f'- Discovery Speedup: {perf.get(\"discovery_speedup\", 1):.1f}x')
print(f'- Execution Speedup: {perf.get(\"execution_speedup\", 1):.1f}x') 
print(f'- Compatibility Score: {compat.get(\"compatibility_score\", 0):.1%}')
print(f'- Tests Found: {data.get(\"fastest_discovery\", {}).get(\"test_count\", 0)}')
          " >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload comparison results
      uses: actions/upload-artifact@v4
      with:
        name: comparison-results-py${{ matrix.python-version }}-size${{ matrix.test-size }}
        path: |
          comparison_results/
          performance_data/
          performance_report.md
        retention-days: 30
        
    - name: Check for performance regressions
      run: |
        # Check for regressions (will exit with non-zero code if critical regressions found)
        python scripts/track_performance_regression.py --check-only || echo "Performance regression detected"

  compatibility-tests:
    name: Compatibility Test Suite
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install pytest pytest-asyncio
        
    - name: Build fastest
      run: cargo build --release
      
    - name: Run compatibility tests with fastest
      run: |
        ./target/release/fastest-cli tests/compatibility/ -v || echo "Some compatibility tests failed"
        
    - name: Run compatibility tests with pytest
      run: |
        pytest tests/compatibility/ -v || echo "Some pytest tests failed"
        
    - name: Compare results
      run: |
        echo "## Compatibility Test Results" >> $GITHUB_STEP_SUMMARY
        echo "Fastest and pytest results compared above" >> $GITHUB_STEP_SUMMARY

  benchmark-real-world:
    name: Real-world Benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Rust and Python
      uses: dtolnay/rust-toolchain@stable
    - uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Build fastest
      run: cargo build --release
      
    - name: Clone test repositories
      run: |
        # Clone some real Python projects for testing
        mkdir -p test_repos
        cd test_repos
        
        # Clone requests (lightweight)
        git clone --depth 1 https://github.com/psf/requests.git
        cd requests
        pip install -e .
        pip install pytest
        cd ..
        
    - name: Benchmark real repositories
      run: |
        # Test on requests repository
        echo "## Real-world Benchmark Results" >> $GITHUB_STEP_SUMMARY
        
        cd test_repos/requests
        echo "### Requests Library" >> $GITHUB_STEP_SUMMARY
        
        # Run with fastest
        echo "Running with fastest..." 
        time ./../../target/release/fastest-cli tests/ > ../../fastest_output.txt 2>&1 || true
        
        # Run with pytest  
        echo "Running with pytest..."
        time pytest tests/ > ../../pytest_output.txt 2>&1 || true
        
        cd ../..
        
        echo "Fastest output:" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        head -20 fastest_output.txt >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        echo "pytest output:" >> $GITHUB_STEP_SUMMARY  
        echo '```' >> $GITHUB_STEP_SUMMARY
        head -20 pytest_output.txt >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY

  report-summary:
    name: Generate Summary Report
    runs-on: ubuntu-latest
    needs: [compare-performance, compatibility-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
        
    - name: Generate summary report
      run: |
        echo "# Fastest vs pytest Comparison Summary" > summary_report.md
        echo "Generated on: $(date)" >> summary_report.md
        echo "" >> summary_report.md
        
        echo "## Performance Results" >> summary_report.md
        
        # Process comparison results from artifacts
        find artifacts/ -name "latest.json" | while read file; do
          echo "Processing $file"
          python -c "
import json
import os
try:
    with open('$file', 'r') as f:
        data = json.load(f)
    
    perf = data.get('performance_ratio', {})
    compat = data.get('compatibility_analysis', {})
    
    print(f'- Discovery: {perf.get(\"discovery_speedup\", 1):.1f}x faster')
    print(f'- Execution: {perf.get(\"execution_speedup\", 1):.1f}x faster')
    print(f'- Compatibility: {compat.get(\"compatibility_score\", 0):.1%}')
    print()
except Exception as e:
    print(f'Error processing {os.path.basename(\"$file\")}: {e}')
          " >> summary_report.md
        done
        
        echo "## Test Matrix Results" >> summary_report.md
        echo "All test combinations completed. Check individual job results for details." >> summary_report.md
        
    - name: Upload summary report
      uses: actions/upload-artifact@v4
      with:
        name: summary-report
        path: summary_report.md
        retention-days: 90