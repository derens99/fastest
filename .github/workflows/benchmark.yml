name: Benchmark

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    
    - name: Install Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Cache cargo index
      uses: actions/cache@v3
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Cache cargo build
      uses: actions/cache@v3
      with:
        path: target
        key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Build Rust binary
      run: |
        cargo build --release
        # Also build debug version which benchmarks expect
        cargo build
    
    - name: Install dependencies
      run: |
        python -m venv venv
        source venv/bin/activate
        pip install pytest maturin
        cd crates/fastest-python && maturin develop --release && cd ../..
    
    - name: Run benchmarks
      run: |
        source venv/bin/activate
        
        # First run the format benchmark to generate JSON for detailed analysis
        python benchmarks/format_benchmark_json.py > benchmark_results_detailed.json || echo '{"error": "Failed to run benchmarks"}' > benchmark_results_detailed.json
        
        # Generate simplified JSON for GitHub benchmark action
        python benchmarks/format_benchmark_github.py > benchmark_results.json || echo '[{"name": "Benchmark Status", "unit": "success", "value": 0}]' > benchmark_results.json
        
        # Also create markdown report
        echo "# Benchmark Results ðŸ“Š" > benchmark_results.md
        echo "" >> benchmark_results.md
        echo "## Summary" >> benchmark_results.md
        echo "" >> benchmark_results.md
        
        # Extract key metrics from detailed JSON
        python -c "
        import json
        with open('benchmark_results_detailed.json') as f:
            data = json.load(f)
            if 'error' in data:
                print('**Error:** Failed to run benchmarks')
            else:
                summary = data.get('summary', {})
                print(f\"- **Discovery**: {summary.get('discovery_speedup', 0):.1f}x faster than pytest\")
                print(f\"- **AST Parser**: Up to {summary.get('best_parser_speedup', 0):.1f}x faster than regex\")
                print(f\"- **Parallel Execution**: Up to {summary.get('best_parallel_speedup', 0):.1f}x speedup\")
        " >> benchmark_results.md
        
        echo "" >> benchmark_results.md
        echo "## Detailed Results" >> benchmark_results.md
        echo "" >> benchmark_results.md
        
        # Run individual benchmarks for detailed output
        echo "### Discovery Benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        python benchmarks/benchmark.py >> benchmark_results.md 2>&1 || echo "Error running discovery benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        
        echo "" >> benchmark_results.md
        echo "### Parser Benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        python benchmarks/benchmark_parsers.py >> benchmark_results.md 2>&1 || echo "Error running parser benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        
        echo "" >> benchmark_results.md
        echo "### Parallel Execution Benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        python benchmarks/benchmark_parallel.py >> benchmark_results.md 2>&1 || echo "Error running parallel benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        
        echo "" >> benchmark_results.md
        echo "### Cache Benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
        python benchmarks/benchmark_cache.py >> benchmark_results.md 2>&1 || echo "Error running cache benchmark" >> benchmark_results.md
        echo '```' >> benchmark_results.md
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark_results.json
          benchmark_results_detailed.json
          benchmark_results.md
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const results = fs.readFileSync('benchmark_results.md', 'utf8');
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const botComment = comments.find(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('Benchmark Results')
          );
          
          if (botComment) {
            // Update existing comment
            github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: results
            });
          } else {
            // Create new comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: results
            });
          }
    
    - name: Store benchmark result
      if: github.ref == 'refs/heads/main'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'customBiggerIsBetter'
        output-file-path: benchmark_results.json
        benchmark-data-dir-path: 'dev/bench'
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true 